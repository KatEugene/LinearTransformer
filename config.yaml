globals:
  seed: 42

model:
  _target_: src.model.LanguageModel
  d_model: 512
  num_heads: 8
  feature_dim: 64
  window_size: 64
  num_layers: 6
  max_seq_len: ${dataloader.max_seq_len}
  dropout: 0.1

loss_function:
  _target_: src.metrics.CrossEntropyLossWrapper
  label_smoothing: 0.1

metrics:
  train:
    - _target_: src.metrics.GradNormMetric
      is_global: False
      name: 'grad_norm'

  inference: []

optimizer:
  _target_: torch.optim.Adam
  lr: 3e-4
  betas: [0.9, 0.98]
  eps: 1e-9

dataloader:
  batch_size: 2
  max_seq_len: 512
  train_size: 3
  max_vocab_size: 16000

lr_scheduler:
  _target_: src.scheduler.WarmUpScheduler
  d_model: ${model.d_model}
  warmup_steps: 15

wandb_tracker:
  _target_: src.metrics.WandbTracker
  project_name: 'VK-LinearTransformer-TestTask'
  run_name: 'baseline'
  mode: 'disabled'
  loss_names: ['loss']
  run_id: null

transforms:
  train: {}
  inference: {}

trainer:
  log_period: 50
  epoch_period: 5
  save_period: 5
  n_epochs: 50
  resume_from: null
  early_stop: null
  to_device: ['input_ids', 'attention_mask']
  checkpoint_dir: 'checkpoints'

inferencer:
  save_dir: 'result'
  pretrained_path: null
